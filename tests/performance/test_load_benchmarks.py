# ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ãƒ†ã‚¹ãƒˆã¨ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯ - Production Load Testing

import pytest
import asyncio
import time
import statistics
import json
import psutil
import threading
from typing import Dict, List, Optional
from dataclasses import dataclass
import numpy as np
import os

# ãƒ†ã‚¹ãƒˆå¯¾è±¡ã®imports
from api.services.chat_service import ChatService
from tests.integration.test_azure_services_integration import AdvancedAzureOpenAIMock

# ========================================
# ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ãƒ†ã‚¹ãƒˆç”¨ãƒ‡ãƒ¼ã‚¿ã‚¯ãƒ©ã‚¹
# ========================================


@dataclass
class LoadTestResult:
    """è² è·ãƒ†ã‚¹ãƒˆçµæœ"""

    test_name: str
    concurrent_users: int
    total_requests: int
    successful_requests: int
    failed_requests: int
    total_duration: float
    avg_response_time: float
    min_response_time: float
    max_response_time: float
    p50_response_time: float
    p95_response_time: float
    p99_response_time: float
    throughput: float  # requests per second
    error_rate: float
    cpu_usage_avg: float
    memory_usage_avg: float
    memory_peak: float


@dataclass
class BenchmarkTarget:
    """ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯ç›®æ¨™å€¤"""

    max_avg_response_time: float = 3.0  # 3ç§’
    max_p95_response_time: float = 5.0  # 5ç§’
    max_p99_response_time: float = 8.0  # 8ç§’
    min_throughput: float = 10.0  # 10 req/sec
    max_error_rate: float = 0.05  # 5%
    max_cpu_usage: float = 80.0  # 80%
    max_memory_usage: float = 2048.0  # 2GB


class SystemResourceMonitor:
    """ã‚·ã‚¹ãƒ†ãƒ ãƒªã‚½ãƒ¼ã‚¹ç›£è¦–"""

    def __init__(self, interval: float = 0.5):
        self.interval = interval
        self.monitoring = False
        self.cpu_readings = []
        self.memory_readings = []
        self.monitor_thread = None

    def start_monitoring(self):
        """ç›£è¦–é–‹å§‹"""
        self.monitoring = True
        self.cpu_readings = []
        self.memory_readings = []
        self.monitor_thread = threading.Thread(target=self._monitor_loop)
        self.monitor_thread.start()

    def stop_monitoring(self) -> Dict[str, float]:
        """ç›£è¦–åœæ­¢ã¨çµæœå–å¾—"""
        self.monitoring = False
        if self.monitor_thread:
            self.monitor_thread.join()

        if not self.cpu_readings or not self.memory_readings:
            return {"cpu_avg": 0.0, "memory_avg": 0.0, "memory_peak": 0.0}

        return {
            "cpu_avg": statistics.mean(self.cpu_readings),
            "memory_avg": statistics.mean(self.memory_readings),
            "memory_peak": max(self.memory_readings),
        }

    def _monitor_loop(self):
        """ç›£è¦–ãƒ«ãƒ¼ãƒ—"""
        process = psutil.Process()

        while self.monitoring:
            try:
                # CPUä½¿ç”¨ç‡
                cpu_percent = process.cpu_percent()
                self.cpu_readings.append(cpu_percent)

                # ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡ï¼ˆMBï¼‰
                memory_mb = process.memory_info().rss / 1024 / 1024
                self.memory_readings.append(memory_mb)

                time.sleep(self.interval)
            except Exception as e:
                print(f"ç›£è¦–ã‚¨ãƒ©ãƒ¼: {e}")
                break


class LoadTestScenario:
    """è² è·ãƒ†ã‚¹ãƒˆã‚·ãƒŠãƒªã‚ª"""

    def __init__(
        self, name: str, queries: List[str], concurrent_users: int, duration: int
    ):
        self.name = name
        self.queries = queries
        self.concurrent_users = concurrent_users
        self.duration = duration  # ç§’
        self.results = []
        self.errors = []


# ========================================
# è² è·ãƒ†ã‚¹ãƒˆã‚¹ã‚¤ãƒ¼ãƒˆ
# ========================================


class TestLoadPerformance:
    """è² è·ãƒ†ã‚¹ãƒˆã¨ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ãƒ†ã‚¹ãƒˆ"""

    @pytest.fixture
    def chat_service_with_mock(self):
        """ãƒ¢ãƒƒã‚¯ä»˜ããƒãƒ£ãƒƒãƒˆã‚µãƒ¼ãƒ“ã‚¹"""
        service = ChatService()
        mock_openai = AdvancedAzureOpenAIMock()
        mock_openai.enable_latency_simulation(0.5)  # 500ms base latency
        service.openai_service = mock_openai
        return service, mock_openai

    @pytest.fixture
    def load_test_scenarios(self):
        """è² è·ãƒ†ã‚¹ãƒˆã‚·ãƒŠãƒªã‚ªã®å®šç¾©"""
        return [
            LoadTestScenario(
                name="light_load",
                queries=[
                    "Azure OpenAIã®åŸºæœ¬çš„ãªä½¿ã„æ–¹ã¯ï¼Ÿ",
                    "RAGã‚·ã‚¹ãƒ†ãƒ ã®æ¦‚è¦ã‚’æ•™ãˆã¦",
                    "æ©Ÿæ¢°å­¦ç¿’ã®åŸºç¤ã«ã¤ã„ã¦",
                    "ã‚¯ãƒ©ã‚¦ãƒ‰ã‚µãƒ¼ãƒ“ã‚¹ã®ãƒ¡ãƒªãƒƒãƒˆã¯ï¼Ÿ",
                ],
                concurrent_users=5,
                duration=30,
            ),
            LoadTestScenario(
                name="medium_load",
                queries=[
                    "Azure OpenAIã®è©³ç´°ãªè¨­å®šæ–¹æ³•",
                    "RAGã‚·ã‚¹ãƒ†ãƒ ã®å®Ÿè£…ã§æ³¨æ„ã™ã¹ãç‚¹",
                    "ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹è¨­è¨ˆã®ãƒ™ã‚¹ãƒˆãƒ—ãƒ©ã‚¯ãƒ†ã‚£ã‚¹",
                    "ã‚»ã‚­ãƒ¥ãƒªãƒ†ã‚£å¯¾ç­–ã®å…·ä½“çš„ãªæ–¹æ³•",
                    "ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°ã®ãƒ†ã‚¯ãƒ‹ãƒƒã‚¯",
                ],
                concurrent_users=15,
                duration=60,
            ),
            LoadTestScenario(
                name="heavy_load",
                queries=[
                    "è¤‡é›‘ãªã‚·ã‚¹ãƒ†ãƒ çµ±åˆã«ãŠã‘ã‚‹èª²é¡Œã¨è§£æ±ºç­–",
                    "å¤§è¦æ¨¡ãƒ‡ãƒ¼ã‚¿å‡¦ç†ã®ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£è¨­è¨ˆ",
                    "ãƒã‚¤ã‚¯ãƒ­ã‚µãƒ¼ãƒ“ã‚¹é–“ã®é€šä¿¡æœ€é©åŒ–",
                    "åˆ†æ•£ã‚·ã‚¹ãƒ†ãƒ ã§ã®ä¸€è²«æ€§ä¿è¨¼",
                    "ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ åˆ†æã‚·ã‚¹ãƒ†ãƒ ã®æ§‹ç¯‰æ–¹æ³•",
                    "æ©Ÿæ¢°å­¦ç¿’ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ã®é‹ç”¨ç›£è¦–",
                    "ã‚»ã‚­ãƒ¥ãƒªãƒ†ã‚£ã‚¤ãƒ³ã‚·ãƒ‡ãƒ³ãƒˆå¯¾å¿œãƒ—ãƒ­ã‚»ã‚¹",
                ],
                concurrent_users=30,
                duration=90,
            ),
        ]

    @pytest.mark.load
    async def test_progressive_load_testing(
        self, chat_service_with_mock, load_test_scenarios
    ):
        """æ®µéšçš„è² è·ãƒ†ã‚¹ãƒˆ"""
        chat_service, mock_openai = chat_service_with_mock
        benchmark = BenchmarkTarget()

        results = []

        for scenario in load_test_scenarios:
            print(f"\n=== {scenario.name.upper()} è² è·ãƒ†ã‚¹ãƒˆé–‹å§‹ ===")
            print(f"ä¸¦è¡Œãƒ¦ãƒ¼ã‚¶ãƒ¼æ•°: {scenario.concurrent_users}")
            print(f"ãƒ†ã‚¹ãƒˆæœŸé–“: {scenario.duration}ç§’")

            # ã‚·ã‚¹ãƒ†ãƒ ãƒªã‚½ãƒ¼ã‚¹ç›£è¦–é–‹å§‹
            monitor = SystemResourceMonitor()
            monitor.start_monitoring()

            # è² è·ãƒ†ã‚¹ãƒˆå®Ÿè¡Œ
            result = await self._execute_load_test(chat_service, scenario)

            # ç›£è¦–åœæ­¢
            resource_metrics = monitor.stop_monitoring()
            result.cpu_usage_avg = resource_metrics["cpu_avg"]
            result.memory_usage_avg = resource_metrics["memory_avg"]
            result.memory_peak = resource_metrics["memory_peak"]

            results.append(result)

            # çµæœã®è¡¨ç¤ºã¨æ¤œè¨¼
            self._print_load_test_results(result)
            self._verify_performance_targets(result, benchmark)

            # æ¬¡ã®ãƒ†ã‚¹ãƒˆã¾ã§ã®å›å¾©æ™‚é–“
            print("ã‚·ã‚¹ãƒ†ãƒ å›å¾©å¾…æ©Ÿä¸­...")
            await asyncio.sleep(10)

        # å…¨ä½“çµæœã®åˆ†æ
        self._analyze_progressive_results(results)

        return results

    async def _execute_load_test(
        self, chat_service: ChatService, scenario: LoadTestScenario
    ) -> LoadTestResult:
        """è² è·ãƒ†ã‚¹ãƒˆå®Ÿè¡Œ"""

        # ãƒ¯ãƒ¼ã‚«ãƒ¼ã‚¿ã‚¹ã‚¯ã®ä½œæˆ
        async def worker_task(worker_id: int):
            """å€‹åˆ¥ãƒ¯ãƒ¼ã‚«ãƒ¼ã®ã‚¿ã‚¹ã‚¯"""
            worker_results = []
            worker_errors = []

            end_time = time.time() + scenario.duration

            while time.time() < end_time:
                for query in scenario.queries:
                    if time.time() >= end_time:
                        break

                    try:
                        start_time = time.time()
                        response = await chat_service.process_message(
                            f"{query} (Worker {worker_id})"
                        )
                        response_time = time.time() - start_time

                        worker_results.append(
                            {
                                "response_time": response_time,
                                "success": response.success
                                if hasattr(response, "success")
                                else True,
                                "worker_id": worker_id,
                            }
                        )

                    except Exception as e:
                        worker_errors.append(
                            {
                                "error": str(e),
                                "worker_id": worker_id,
                                "timestamp": time.time(),
                            }
                        )

                # çŸ­ã„é–“éš”ã§ã®ãƒªã‚¯ã‚¨ã‚¹ãƒˆé–“éš”
                await asyncio.sleep(0.1)

            return worker_results, worker_errors

        # å…¨ãƒ¯ãƒ¼ã‚«ãƒ¼ã®å®Ÿè¡Œ
        start_time = time.time()
        tasks = [worker_task(i) for i in range(scenario.concurrent_users)]
        worker_results = await asyncio.gather(*tasks, return_exceptions=True)
        total_duration = time.time() - start_time

        # çµæœã®é›†è¨ˆ
        all_results = []
        all_errors = []

        for worker_result in worker_results:
            if isinstance(worker_result, Exception):
                all_errors.append({"error": str(worker_result), "worker_id": "unknown"})
            else:
                results, errors = worker_result
                all_results.extend(results)
                all_errors.extend(errors)

        # çµ±è¨ˆè¨ˆç®—
        response_times = [r["response_time"] for r in all_results]
        successful_requests = sum(1 for r in all_results if r["success"])
        failed_requests = len(all_results) - successful_requests + len(all_errors)

        if response_times:
            avg_response_time = statistics.mean(response_times)
            min_response_time = min(response_times)
            max_response_time = max(response_times)
            p50_response_time = np.percentile(response_times, 50)
            p95_response_time = np.percentile(response_times, 95)
            p99_response_time = np.percentile(response_times, 99)
        else:
            avg_response_time = min_response_time = max_response_time = 0
            p50_response_time = p95_response_time = p99_response_time = 0

        total_requests = len(all_results) + len(all_errors)
        throughput = total_requests / total_duration if total_duration > 0 else 0
        error_rate = failed_requests / total_requests if total_requests > 0 else 0

        return LoadTestResult(
            test_name=scenario.name,
            concurrent_users=scenario.concurrent_users,
            total_requests=total_requests,
            successful_requests=successful_requests,
            failed_requests=failed_requests,
            total_duration=total_duration,
            avg_response_time=avg_response_time,
            min_response_time=min_response_time,
            max_response_time=max_response_time,
            p50_response_time=p50_response_time,
            p95_response_time=p95_response_time,
            p99_response_time=p99_response_time,
            throughput=throughput,
            error_rate=error_rate,
            cpu_usage_avg=0.0,  # å¾Œã§è¨­å®š
            memory_usage_avg=0.0,  # å¾Œã§è¨­å®š
            memory_peak=0.0,  # å¾Œã§è¨­å®š
        )

    def _print_load_test_results(self, result: LoadTestResult):
        """è² è·ãƒ†ã‚¹ãƒˆçµæœã®è¡¨ç¤º"""
        print(f"\nğŸ“Š {result.test_name} çµæœ:")
        print(f"   ç·ãƒªã‚¯ã‚¨ã‚¹ãƒˆæ•°: {result.total_requests}")
        print(f"   æˆåŠŸ: {result.successful_requests}, å¤±æ•—: {result.failed_requests}")
        print(f"   æˆåŠŸç‡: {(1-result.error_rate)*100:.1f}%")
        print(f"   ã‚¹ãƒ«ãƒ¼ãƒ—ãƒƒãƒˆ: {result.throughput:.1f} req/sec")
        print(f"   ãƒ¬ã‚¹ãƒãƒ³ã‚¹æ™‚é–“ (å¹³å‡): {result.avg_response_time:.2f}s")
        print(f"   ãƒ¬ã‚¹ãƒãƒ³ã‚¹æ™‚é–“ (P95): {result.p95_response_time:.2f}s")
        print(f"   ãƒ¬ã‚¹ãƒãƒ³ã‚¹æ™‚é–“ (P99): {result.p99_response_time:.2f}s")
        print(f"   CPUä½¿ç”¨ç‡ (å¹³å‡): {result.cpu_usage_avg:.1f}%")
        print(f"   ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡ (å¹³å‡): {result.memory_usage_avg:.1f}MB")
        print(f"   ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡ (ãƒ”ãƒ¼ã‚¯): {result.memory_peak:.1f}MB")

    def _verify_performance_targets(
        self, result: LoadTestResult, benchmark: BenchmarkTarget
    ):
        """ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ç›®æ¨™ã®æ¤œè¨¼"""
        issues = []

        if result.avg_response_time > benchmark.max_avg_response_time:
            issues.append(
                f"å¹³å‡ãƒ¬ã‚¹ãƒãƒ³ã‚¹æ™‚é–“ãŒç›®æ¨™ã‚’è¶…é: {result.avg_response_time:.2f}s > {benchmark.max_avg_response_time}s"
            )

        if result.p95_response_time > benchmark.max_p95_response_time:
            issues.append(
                f"P95ãƒ¬ã‚¹ãƒãƒ³ã‚¹æ™‚é–“ãŒç›®æ¨™ã‚’è¶…é: {result.p95_response_time:.2f}s > {benchmark.max_p95_response_time}s"
            )

        if result.p99_response_time > benchmark.max_p99_response_time:
            issues.append(
                f"P99ãƒ¬ã‚¹ãƒãƒ³ã‚¹æ™‚é–“ãŒç›®æ¨™ã‚’è¶…é: {result.p99_response_time:.2f}s > {benchmark.max_p99_response_time}s"
            )

        if result.throughput < benchmark.min_throughput:
            issues.append(
                f"ã‚¹ãƒ«ãƒ¼ãƒ—ãƒƒãƒˆãŒç›®æ¨™ã‚’ä¸‹å›ã‚‹: {result.throughput:.1f} < {benchmark.min_throughput} req/sec"
            )

        if result.error_rate > benchmark.max_error_rate:
            issues.append(
                f"ã‚¨ãƒ©ãƒ¼ç‡ãŒç›®æ¨™ã‚’è¶…é: {result.error_rate*100:.1f}% > {benchmark.max_error_rate*100}%"
            )

        if result.cpu_usage_avg > benchmark.max_cpu_usage:
            issues.append(
                f"CPUä½¿ç”¨ç‡ãŒç›®æ¨™ã‚’è¶…é: {result.cpu_usage_avg:.1f}% > {benchmark.max_cpu_usage}%"
            )

        if result.memory_peak > benchmark.max_memory_usage:
            issues.append(
                f"ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡ãŒç›®æ¨™ã‚’è¶…é: {result.memory_peak:.1f}MB > {benchmark.max_memory_usage}MB"
            )

        if issues:
            print("\nâš ï¸  ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹èª²é¡Œ:")
            for issue in issues:
                print(f"   - {issue}")

            # ãƒ†ã‚¹ãƒˆå¤±æ•—ï¼ˆè»½è² è·ä»¥å¤–ï¼‰
            if result.test_name != "light_load":
                pytest.fail(f"ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ç›®æ¨™æœªé”: {', '.join(issues)}")
        else:
            print(f"\nâœ… {result.test_name}: å…¨ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ç›®æ¨™ã‚’ã‚¯ãƒªã‚¢")

    def _analyze_progressive_results(self, results: List[LoadTestResult]):
        """æ®µéšçš„çµæœã®åˆ†æ"""
        print("\nğŸ“ˆ æ®µéšçš„è² è·ãƒ†ã‚¹ãƒˆç·åˆåˆ†æ:")
        print(
            f"{'ãƒ†ã‚¹ãƒˆå':<15} {'ãƒ¦ãƒ¼ã‚¶ãƒ¼æ•°':<8} {'ã‚¹ãƒ«ãƒ¼ãƒ—ãƒƒãƒˆ':<12} {'P95æ™‚é–“':<10} {'ã‚¨ãƒ©ãƒ¼ç‡':<8} {'CPU%':<8}"
        )
        print("-" * 70)

        for result in results:
            print(
                f"{result.test_name:<15} {result.concurrent_users:<8} "
                f"{result.throughput:<12.1f} {result.p95_response_time:<10.2f} "
                f"{result.error_rate*100:<8.1f} {result.cpu_usage_avg:<8.1f}"
            )

        # ã‚¹ã‚±ãƒ¼ãƒ©ãƒ“ãƒªãƒ†ã‚£åˆ†æ
        if len(results) >= 2:
            throughput_improvement = results[-1].throughput / results[0].throughput
            response_degradation = (
                results[-1].p95_response_time / results[0].p95_response_time
            )

            print("\nã‚¹ã‚±ãƒ¼ãƒ©ãƒ“ãƒªãƒ†ã‚£åˆ†æ:")
            print(f"  ã‚¹ãƒ«ãƒ¼ãƒ—ãƒƒãƒˆå‘ä¸Šç‡: {throughput_improvement:.2f}x")
            print(f"  ãƒ¬ã‚¹ãƒãƒ³ã‚¹åŠ£åŒ–ç‡: {response_degradation:.2f}x")

            if throughput_improvement > 2.0 and response_degradation < 3.0:
                print("  âœ… è‰¯å¥½ãªã‚¹ã‚±ãƒ¼ãƒ©ãƒ“ãƒªãƒ†ã‚£")
            else:
                print("  âš ï¸  ã‚¹ã‚±ãƒ¼ãƒ©ãƒ“ãƒªãƒ†ã‚£ã«èª²é¡Œã‚ã‚Š")


# ========================================
# ãƒ¡ãƒ¢ãƒªãƒªãƒ¼ã‚¯ãƒ†ã‚¹ãƒˆ
# ========================================


class TestMemoryLeakDetection:
    """ãƒ¡ãƒ¢ãƒªãƒªãƒ¼ã‚¯ã®æ¤œå‡ºãƒ†ã‚¹ãƒˆ"""

    @pytest.mark.memory
    async def test_memory_leak_detection(self, chat_service_with_mock):
        """é•·æ™‚é–“å®Ÿè¡Œã«ã‚ˆã‚‹ãƒ¡ãƒ¢ãƒªãƒªãƒ¼ã‚¯æ¤œå‡º"""
        chat_service, mock_openai = chat_service_with_mock

        # åˆæœŸãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡
        process = psutil.Process()
        initial_memory = process.memory_info().rss / 1024 / 1024  # MB

        memory_readings = [initial_memory]
        test_queries = [
            "ãƒ¡ãƒ¢ãƒªãƒªãƒ¼ã‚¯ãƒ†ã‚¹ãƒˆè³ªå•1",
            "ãƒ¡ãƒ¢ãƒªãƒªãƒ¼ã‚¯ãƒ†ã‚¹ãƒˆè³ªå•2",
            "ãƒ¡ãƒ¢ãƒªãƒªãƒ¼ã‚¯ãƒ†ã‚¹ãƒˆè³ªå•3",
        ]

        # 500å›ã®ãƒªã‚¯ã‚¨ã‚¹ãƒˆã‚’å®Ÿè¡Œ
        for i in range(500):
            query = test_queries[i % len(test_queries)]
            await chat_service.process_message(f"{query} - Iteration {i}")

            # 50å›ã”ã¨ã«ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡ã‚’è¨˜éŒ²
            if i % 50 == 0:
                current_memory = process.memory_info().rss / 1024 / 1024
                memory_readings.append(current_memory)
                print(f"Iteration {i}: Memory usage {current_memory:.1f}MB")

                # ã‚¬ãƒ™ãƒ¼ã‚¸ã‚³ãƒ¬ã‚¯ã‚·ãƒ§ãƒ³
                import gc

                gc.collect()

        # æœ€çµ‚ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡
        final_memory = process.memory_info().rss / 1024 / 1024
        memory_readings.append(final_memory)

        # ãƒ¡ãƒ¢ãƒªå¢—åŠ ã®åˆ†æ
        memory_increase = final_memory - initial_memory
        max_memory = max(memory_readings)

        print("\nãƒ¡ãƒ¢ãƒªãƒªãƒ¼ã‚¯åˆ†æ:")
        print(f"  åˆæœŸãƒ¡ãƒ¢ãƒª: {initial_memory:.1f}MB")
        print(f"  æœ€çµ‚ãƒ¡ãƒ¢ãƒª: {final_memory:.1f}MB")
        print(f"  æœ€å¤§ãƒ¡ãƒ¢ãƒª: {max_memory:.1f}MB")
        print(f"  ãƒ¡ãƒ¢ãƒªå¢—åŠ : {memory_increase:.1f}MB")

        # ãƒ¡ãƒ¢ãƒªãƒªãƒ¼ã‚¯ã®åˆ¤å®š
        # åŸºæº–: 500ãƒªã‚¯ã‚¨ã‚¹ãƒˆå¾Œã®ãƒ¡ãƒ¢ãƒªå¢—åŠ ãŒ200MBä»¥ä¸‹
        assert memory_increase < 200, f"ãƒ¡ãƒ¢ãƒªãƒªãƒ¼ã‚¯æ¤œå‡º: {memory_increase:.1f}MBå¢—åŠ "

        # ç·šå½¢çš„ãªãƒ¡ãƒ¢ãƒªå¢—åŠ ãŒãªã„ã“ã¨ã‚’ç¢ºèª
        if len(memory_readings) > 3:
            # æœ€å¾Œã®3ã¤ã®èª­ã¿å–ã‚Šå€¤ã®å‚¾å‘ã‚’åˆ†æ
            recent_readings = memory_readings[-3:]
            trend = (recent_readings[-1] - recent_readings[0]) / len(recent_readings)

            assert trend < 10, f"ç¶™ç¶šçš„ãªãƒ¡ãƒ¢ãƒªå¢—åŠ å‚¾å‘ã‚’æ¤œå‡º: {trend:.1f}MB/reading"

        print("âœ… ãƒ¡ãƒ¢ãƒªãƒªãƒ¼ã‚¯ãƒ†ã‚¹ãƒˆåˆæ ¼")


# ========================================
# ã‚¹ãƒˆãƒ¬ã‚¹ãƒ†ã‚¹ãƒˆ
# ========================================


class TestStressTesting:
    """ã‚¹ãƒˆãƒ¬ã‚¹ãƒ†ã‚¹ãƒˆã¨ã‚¨ãƒ©ãƒ¼å›å¾©åŠ›ãƒ†ã‚¹ãƒˆ"""

    @pytest.mark.stress
    async def test_extreme_load_stress(self, chat_service_with_mock):
        """æ¥µé™è² è·ã‚¹ãƒˆãƒ¬ã‚¹ãƒ†ã‚¹ãƒˆ"""
        chat_service, mock_openai = chat_service_with_mock

        # é«˜ã‚¨ãƒ©ãƒ¼ç‡ã¨ãƒ¬ã‚¤ãƒ†ãƒ³ã‚·ã‚’è¨­å®š
        mock_openai.set_error_rate(0.3)  # 30%ã‚¨ãƒ©ãƒ¼ç‡
        mock_openai.enable_latency_simulation(2.0)  # 2ç§’ãƒ™ãƒ¼ã‚¹ãƒ¬ã‚¤ãƒ†ãƒ³ã‚·

        # 100ä¸¦è¡Œãƒªã‚¯ã‚¨ã‚¹ãƒˆã§5åˆ†é–“
        concurrent_requests = 100
        test_duration = 300  # 5åˆ†é–“

        print(f"æ¥µé™ã‚¹ãƒˆãƒ¬ã‚¹ãƒ†ã‚¹ãƒˆé–‹å§‹: {concurrent_requests}ä¸¦è¡Œ, {test_duration}ç§’é–“")

        async def stress_worker(worker_id: int):
            """ã‚¹ãƒˆãƒ¬ã‚¹ãƒ†ã‚¹ãƒˆç”¨ãƒ¯ãƒ¼ã‚«ãƒ¼"""
            requests_count = 0
            errors_count = 0
            end_time = time.time() + test_duration

            while time.time() < end_time:
                try:
                    await chat_service.process_message(
                        f"ã‚¹ãƒˆãƒ¬ã‚¹ãƒ†ã‚¹ãƒˆè³ªå• Worker {worker_id}"
                    )
                    requests_count += 1
                except Exception:
                    errors_count += 1

                # ãƒªã‚¯ã‚¨ã‚¹ãƒˆé–“éš”ãªã—ï¼ˆæœ€å¤§è² è·ï¼‰

            return requests_count, errors_count

        # ã‚¹ãƒˆãƒ¬ã‚¹ãƒ†ã‚¹ãƒˆå®Ÿè¡Œ
        start_time = time.time()
        tasks = [stress_worker(i) for i in range(concurrent_requests)]

        # ã‚·ã‚¹ãƒ†ãƒ ãƒªã‚½ãƒ¼ã‚¹ç›£è¦–
        monitor = SystemResourceMonitor(interval=1.0)
        monitor.start_monitoring()

        try:
            results = await asyncio.gather(*tasks, return_exceptions=True)
        finally:
            resource_metrics = monitor.stop_monitoring()

        execution_time = time.time() - start_time

        # çµæœé›†è¨ˆ
        total_requests = 0
        total_errors = 0

        for result in results:
            if isinstance(result, Exception):
                total_errors += 1
            else:
                requests, errors = result
                total_requests += requests
                total_errors += errors

        # ã‚¹ãƒˆãƒ¬ã‚¹ãƒ†ã‚¹ãƒˆçµæœ
        error_rate = (
            total_errors / (total_requests + total_errors)
            if (total_requests + total_errors) > 0
            else 1.0
        )
        throughput = total_requests / execution_time

        print("\nğŸ”¥ ã‚¹ãƒˆãƒ¬ã‚¹ãƒ†ã‚¹ãƒˆçµæœ:")
        print(f"   å®Ÿè¡Œæ™‚é–“: {execution_time:.1f}ç§’")
        print(f"   ç·ãƒªã‚¯ã‚¨ã‚¹ãƒˆ: {total_requests}")
        print(f"   ç·ã‚¨ãƒ©ãƒ¼: {total_errors}")
        print(f"   ã‚¨ãƒ©ãƒ¼ç‡: {error_rate*100:.1f}%")
        print(f"   ã‚¹ãƒ«ãƒ¼ãƒ—ãƒƒãƒˆ: {throughput:.1f} req/sec")
        print(f"   CPUå¹³å‡ä½¿ç”¨ç‡: {resource_metrics['cpu_avg']:.1f}%")
        print(f"   ãƒ¡ãƒ¢ãƒªãƒ”ãƒ¼ã‚¯: {resource_metrics['memory_peak']:.1f}MB")

        # ã‚¹ãƒˆãƒ¬ã‚¹ãƒ†ã‚¹ãƒˆåŸºæº–
        # ã‚·ã‚¹ãƒ†ãƒ ãŒå®Œå…¨ã«ã‚¯ãƒ©ãƒƒã‚·ãƒ¥ã—ãªã„ã“ã¨ãŒé‡è¦
        assert total_requests > 0, "ã‚·ã‚¹ãƒ†ãƒ ãŒå®Œå…¨ã«å¿œç­”ä¸èƒ½"
        assert error_rate < 0.8, f"ã‚¨ãƒ©ãƒ¼ç‡ãŒéåº¦ã«é«˜ã„: {error_rate*100:.1f}%"
        assert resource_metrics["cpu_avg"] < 100, "CPUä½¿ç”¨ç‡ãŒ100%ã‚’è¶…é"

        print("âœ… ã‚¹ãƒˆãƒ¬ã‚¹ãƒ†ã‚¹ãƒˆåˆæ ¼: ã‚·ã‚¹ãƒ†ãƒ ã¯æ¥µé™è² è·ä¸‹ã§ã‚‚å‹•ä½œç¶™ç¶š")


# ========================================
# ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯æ¯”è¼ƒãƒ†ã‚¹ãƒˆ
# ========================================


class TestBenchmarkComparison:
    """ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯æ¯”è¼ƒã¨ãƒªã‚°ãƒ¬ãƒƒã‚·ãƒ§ãƒ³æ¤œå‡º"""

    BENCHMARK_FILE = "tests/benchmarks/performance_baseline.json"

    @pytest.mark.benchmark
    async def test_performance_regression(self, chat_service_with_mock):
        """ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹å›å¸°ãƒ†ã‚¹ãƒˆ"""
        chat_service, mock_openai = chat_service_with_mock

        # æ¨™æº–ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯ã‚¯ã‚¨ãƒª
        benchmark_queries = [
            "Azure OpenAIã®åŸºæœ¬çš„ãªä½¿ã„æ–¹",
            "RAGã‚·ã‚¹ãƒ†ãƒ ã®æ¦‚è¦èª¬æ˜",
            "ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹è¨­è¨ˆåŸå‰‡",
            "ã‚»ã‚­ãƒ¥ãƒªãƒ†ã‚£ãƒ™ã‚¹ãƒˆãƒ—ãƒ©ã‚¯ãƒ†ã‚£ã‚¹",
            "ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹æœ€é©åŒ–æ‰‹æ³•",
        ]

        # ç¾åœ¨ã®ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹æ¸¬å®š
        current_results = []

        for query in benchmark_queries:
            times = []
            for _ in range(10):  # å„ã‚¯ã‚¨ãƒªã‚’10å›å®Ÿè¡Œ
                start_time = time.time()
                await chat_service.process_message(query)
                response_time = time.time() - start_time
                times.append(response_time)

            current_results.append(
                {
                    "query": query,
                    "avg_time": statistics.mean(times),
                    "p95_time": np.percentile(times, 95),
                    "min_time": min(times),
                    "max_time": max(times),
                }
            )

        # ãƒ™ãƒ¼ã‚¹ãƒ©ã‚¤ãƒ³æ¯”è¼ƒ
        baseline = self._load_baseline()
        regression_detected = False

        print("\nğŸ“Š ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹å›å¸°åˆ†æ:")
        print(f"{'ã‚¯ã‚¨ãƒª':<30} {'ç¾åœ¨':<10} {'ãƒ™ãƒ¼ã‚¹':<10} {'å¤‰åŒ–':<10} {'åˆ¤å®š'}")
        print("-" * 70)

        for i, result in enumerate(current_results):
            current_time = result["avg_time"]

            if baseline and i < len(baseline):
                baseline_time = baseline[i]["avg_time"]
                change_percent = ((current_time - baseline_time) / baseline_time) * 100

                if change_percent > 20:  # 20%ä»¥ä¸Šã®åŠ£åŒ–
                    status = "âŒ å›å¸°"
                    regression_detected = True
                elif change_percent < -10:  # 10%ä»¥ä¸Šã®æ”¹å–„
                    status = "âœ… æ”¹å–„"
                else:
                    status = "â– å¤‰åŒ–ãªã—"

                print(
                    f"{result['query'][:30]:<30} {current_time:<10.2f} {baseline_time:<10.2f} "
                    f"{change_percent:<10.1f}% {status}"
                )
            else:
                print(
                    f"{result['query'][:30]:<30} {current_time:<10.2f} {'N/A':<10} {'N/A':<10} {'æ–°è¦'}"
                )

        # æ–°ã—ã„ãƒ™ãƒ¼ã‚¹ãƒ©ã‚¤ãƒ³ã¨ã—ã¦ä¿å­˜
        self._save_baseline(current_results)

        # å›å¸°æ¤œå‡ºæ™‚ã¯ãƒ†ã‚¹ãƒˆå¤±æ•—
        if regression_detected:
            pytest.fail("ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹å›å¸°ãŒæ¤œå‡ºã•ã‚Œã¾ã—ãŸ")
        else:
            print("\nâœ… ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹å›å¸°ãªã—")

    def _load_baseline(self) -> Optional[List[Dict]]:
        """ãƒ™ãƒ¼ã‚¹ãƒ©ã‚¤ãƒ³çµæœã®èª­ã¿è¾¼ã¿"""
        try:
            os.makedirs(os.path.dirname(self.BENCHMARK_FILE), exist_ok=True)
            with open(self.BENCHMARK_FILE, "r") as f:
                return json.load(f)
        except FileNotFoundError:
            return None
        except Exception as e:
            print(f"ãƒ™ãƒ¼ã‚¹ãƒ©ã‚¤ãƒ³èª­ã¿è¾¼ã¿ã‚¨ãƒ©ãƒ¼: {e}")
            return None

    def _save_baseline(self, results: List[Dict]):
        """ãƒ™ãƒ¼ã‚¹ãƒ©ã‚¤ãƒ³çµæœã®ä¿å­˜"""
        try:
            os.makedirs(os.path.dirname(self.BENCHMARK_FILE), exist_ok=True)
            with open(self.BENCHMARK_FILE, "w") as f:
                json.dump(results, f, indent=2)
        except Exception as e:
            print(f"ãƒ™ãƒ¼ã‚¹ãƒ©ã‚¤ãƒ³ä¿å­˜ã‚¨ãƒ©ãƒ¼: {e}")


# ========================================
# ãƒ†ã‚¹ãƒˆå®Ÿè¡Œè¨­å®š
# ========================================

if __name__ == "__main__":
    pytest.main(
        [
            __file__,
            "-v",
            "--tb=short",
            "-m",
            "load or performance or memory or stress or benchmark",
        ]
    )
