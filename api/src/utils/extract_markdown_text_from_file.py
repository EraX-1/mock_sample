import io
import os
import tempfile
from itertools import groupby

import html2text
from bs4 import BeautifulSoup
from docx import Document
from openpyxl import Workbook, load_workbook
from pptx import Presentation
from pypdf import PdfReader, PdfWriter
from tqdm import tqdm

from src.services.azure_ai_doc_intel import AzureAIDocumentIntelligence
from src.utils.convert_file_to_pdf import convert_image_to_pdf


def extract_markdown_text_from_docx(file: bytes) -> list[dict[str, any]]:
    """Wordãƒ•ã‚¡ã‚¤ãƒ«(.doc, .docx)ã®byteså‹ãƒ‡ãƒ¼ã‚¿ã‹ã‚‰ãƒãƒ¼ã‚¯ãƒ€ã‚¦ãƒ³å½¢å¼ã§ãƒ†ã‚­ã‚¹ãƒˆã‚’æŠ½å‡ºã™ã‚‹

    Args:
        file (bytes): Wordãƒ•ã‚¡ã‚¤ãƒ«(.doc, .docx)ã®byteså‹ãƒ‡ãƒ¼ã‚¿

    Returns:
        list[dict[str, any]]: å„ãƒšãƒ¼ã‚¸ã®ãƒ†ã‚­ã‚¹ãƒˆã¨ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ã‚’å«ã‚€è¾æ›¸ã®ãƒªã‚¹ãƒˆ
    """
    paragraph_contents = []

    def process_paragraph(i_para):
        """å„æ®µè½ã‚’å‡¦ç†ã™ã‚‹"""
        i, paragraph = i_para
        try:
            # TODO: 1ãƒšãƒ¼ã‚¸ã®åŸºæº–ã‚’å®šç¾©ã—ã¦ãƒšãƒ¼ã‚¸ç•ªå·ã‚’ä»˜ä¸ã™ã‚‹å¿…è¦ãŒã‚ã‚‹
            # æ®µè½ã®ãƒ†ã‚­ã‚¹ãƒˆã‚’å–å¾—
            text = paragraph.text.strip()
            if text:
                markdown_text = ""

                # æ®µè½ã‚¹ã‚¿ã‚¤ãƒ«ã«åŸºã¥ã„ã¦ãƒãƒ¼ã‚¯ãƒ€ã‚¦ãƒ³å½¢å¼ã«å¤‰æ›
                if paragraph.style.name.startswith("Heading"):
                    level = int(paragraph.style.name[-1])  # Heading 1, 2, 3ãªã©ã®æ•°å­—ã‚’å–å¾—
                    markdown_text = "#" * level + " " + text + "\n\n"  # ãƒ˜ãƒƒãƒ€ãƒ¼å¾Œã«2ã¤ã®æ”¹è¡Œã‚’è¿½åŠ 
                else:
                    markdown_text = text + "\n\n"  # é€šå¸¸ã®æ®µè½ã‚‚2ã¤ã®æ”¹è¡Œã§åŒºåˆ‡ã‚‹

                return {
                    "content": markdown_text,
                    "page": 1,  # Wordãƒ•ã‚¡ã‚¤ãƒ«ã§ã¯ãƒšãƒ¼ã‚¸ç•ªå·ã®å–å¾—ãŒé›£ã—ã„ãŸã‚å›ºå®šå€¤
                    "order": i,  # æ®µè½ã®é †åºã‚’ä¿æŒ
                }
            return None
        except Exception as e:
            raise Exception(f"æ®µè½ã®å‡¦ç†ä¸­ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ {i}: {e}")

    # Wordãƒ•ã‚¡ã‚¤ãƒ«ã‚’èª­ã¿è¾¼ã‚€
    byte_stream = io.BytesIO(file)
    doc = Document(byte_stream)

    # NOTE: éåŒæœŸå‡¦ç†ã§ã¯ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ãŒä¸ŠãŒã‚‰ãšã€ä¸¦åˆ—ãƒ»ä¸¦è¡Œå‡¦ç†ã¯ä¸Šæ‰‹ãå®Ÿè£…ã§ããªã‹ã£ãŸ
    for i, para in enumerate(doc.paragraphs):
        res = process_paragraph((i, para))
        if res is not None:
            paragraph_contents.append(res)

    # æ®µè½ã®é †åºã§ã‚½ãƒ¼ãƒˆã™ã‚‹
    paragraph_contents.sort(key=lambda x: x["order"])

    # ãƒšãƒ¼ã‚¸ã”ã¨ã«ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã‚’ã¾ã¨ã‚ã‚‹
    docs = []
    for page, group in groupby(paragraph_contents, key=lambda x: x["page"]):
        combined_content = ""
        for item in group:
            combined_content += item["content"]

        docs.append(
            {
                "page_content": combined_content + f"<PAGE_NUMBER>{page}</PAGE_NUMBER>",
                "metadata": {"page": page},
            }
        )

    return docs


def extract_markdown_text_from_pptx(file: bytes) -> list[dict[str, any]]:
    """PowerPointãƒ•ã‚¡ã‚¤ãƒ«(.pptx)ã®byteså‹ãƒ‡ãƒ¼ã‚¿ã‹ã‚‰ãƒãƒ¼ã‚¯ãƒ€ã‚¦ãƒ³å½¢å¼ã§ãƒ†ã‚­ã‚¹ãƒˆã‚’æŠ½å‡ºã™ã‚‹

    Args:
        file (bytes): PowerPointãƒ•ã‚¡ã‚¤ãƒ«(.pptx)ã®byteså‹ãƒ‡ãƒ¼ã‚¿

    Returns:
        list[dict[str, any]]: å„ãƒšãƒ¼ã‚¸ã®ãƒ†ã‚­ã‚¹ãƒˆã¨ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ã‚’å«ã‚€è¾æ›¸ã®ãƒªã‚¹ãƒˆ
    """
    docs = []

    def process_slide(i_slide):
        """å„ã‚¹ãƒ©ã‚¤ãƒ‰ã‚’å‡¦ç†ã™ã‚‹"""
        i, slide = i_slide
        slide_docs = []
        try:
            # ã‚¹ãƒ©ã‚¤ãƒ‰ã®ãƒ†ã‚­ã‚¹ãƒˆã‚’æŠ½å‡º
            texts = []

            # ã‚¿ã‚¤ãƒˆãƒ«ã®æŠ½å‡º
            if slide.shapes.title:
                texts.append(f"# {slide.shapes.title.text}\n")

            # ãã®ä»–ã®ãƒ†ã‚­ã‚¹ãƒˆã®æŠ½å‡º
            for shape in slide.shapes:
                if (
                    hasattr(shape, "text")
                    and shape.text.strip()
                    and shape != slide.shapes.title
                ):
                    # ãƒ†ã‚­ã‚¹ãƒˆãƒœãƒƒã‚¯ã‚¹ã®å ´åˆ
                    text = shape.text.strip()
                    if text:
                        texts.append(text + "\n")

            # ã‚¹ãƒ©ã‚¤ãƒ‰ã‹ã‚‰æŠ½å‡ºã—ãŸãƒ†ã‚­ã‚¹ãƒˆã‚’çµåˆ
            content = "\n".join(texts).strip()

            if content:
                slide_docs.append(
                    {"page_content": content, "metadata": {"page": i + 1}}
                )

            return slide_docs

        except Exception as e:
            raise Exception(f"ã‚¹ãƒ©ã‚¤ãƒ‰ã®å‡¦ç†ä¸­ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ {i}: {e}")

    temp_file = tempfile.NamedTemporaryFile(delete=False)
    try:
        temp_file.write(file)
        temp_file.close()

        # PowerPointãƒ•ã‚¡ã‚¤ãƒ«ã‚’èª­ã¿è¾¼ã‚€
        prs = Presentation(temp_file.name)

        # NOTE: éåŒæœŸå‡¦ç†ã§ã¯ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ãŒä¸ŠãŒã‚‰ãšã€ä¸¦åˆ—ãƒ»ä¸¦è¡Œå‡¦ç†ã¯ä¸Šæ‰‹ãå®Ÿè£…ã§ããªã‹ã£ãŸ
        for i, slide in enumerate(prs.slides):
            docs.extend(process_slide((i, slide)))

    finally:
        os.unlink(temp_file.name)

    # ãƒšãƒ¼ã‚¸ç•ªå·é †ã«ã‚½ãƒ¼ãƒˆã™ã‚‹
    docs = sorted(docs, key=lambda x: x["metadata"]["page"])

    # ãƒšãƒ¼ã‚¸ç•ªå·ã‚’ãƒ†ã‚­ã‚¹ãƒˆã«åŸ‹ã‚è¾¼ã‚€
    for doc in docs:
        doc["page_content"] += (
            "<PAGE_NUMBER>" + str(doc["metadata"]["page"]) + "</PAGE_NUMBER>"
        )

    return docs


def extract_markdown_text_from_html(file: bytes) -> list[dict[str, any]]:
    """HTMLãƒ•ã‚¡ã‚¤ãƒ«(.html)ã®byteså‹ãƒ‡ãƒ¼ã‚¿ã‹ã‚‰ãƒãƒ¼ã‚¯ãƒ€ã‚¦ãƒ³å½¢å¼ã§ãƒ†ã‚­ã‚¹ãƒˆã‚’æŠ½å‡ºã™ã‚‹

    Args:
        file (bytes): HTMLãƒ•ã‚¡ã‚¤ãƒ«(.html)ã®byteså‹ãƒ‡ãƒ¼ã‚¿

    Returns:
        list[dict[str, any]]: å„ãƒšãƒ¼ã‚¸ã®ãƒ†ã‚­ã‚¹ãƒˆã¨ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ã‚’å«ã‚€è¾æ›¸ã®ãƒªã‚¹ãƒˆ
    """
    docs = []

    def process_html_section(i_section):
        """å„ã‚»ã‚¯ã‚·ãƒ§ãƒ³ã‚’å‡¦ç†ã™ã‚‹"""
        i, section = i_section
        section_docs = []
        try:
            h2t = html2text.HTML2Text()
            h2t.ignore_links = False
            h2t.ignore_images = False
            h2t.ignore_tables = False

            # ã‚»ã‚¯ã‚·ãƒ§ãƒ³ã®ãƒ†ã‚­ã‚¹ãƒˆã‚’æŠ½å‡ºã—ã¦ãƒãƒ¼ã‚¯ãƒ€ã‚¦ãƒ³ã«å¤‰æ›
            markdown_content = h2t.handle(str(section))

            # ç©ºã®ã‚»ã‚¯ã‚·ãƒ§ãƒ³ã‚’ã‚¹ã‚­ãƒƒãƒ—
            if markdown_content.strip():
                section_docs.append(
                    {"page_content": markdown_content, "metadata": {"page": i + 1}}
                )

            return section_docs

        except Exception as e:
            raise Exception(f"ã‚»ã‚¯ã‚·ãƒ§ãƒ³ã®å‡¦ç†ä¸­ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ {i}: {e}")

    try:
        # HTMLã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã‚’ãƒ‘ãƒ¼ã‚¹
        soup = BeautifulSoup(file.decode("utf-8"), "html.parser")

        # ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã‚’æ„å‘³ã®ã‚ã‚‹å˜ä½ã§åˆ†å‰²
        # article, section, div ãªã©ã®ä¸»è¦ãªã‚³ãƒ³ãƒ†ãƒŠè¦ç´ ã‚’æ¤œç´¢
        sections = soup.find_all(
            ["article", "section", "div"], class_=True, recursive=False
        )
        if not sections:
            # ã‚»ã‚¯ã‚·ãƒ§ãƒ³ãŒè¦‹ã¤ã‹ã‚‰ãªã„å ´åˆã¯ã€bodyã‚’1ã¤ã®ã‚»ã‚¯ã‚·ãƒ§ãƒ³ã¨ã—ã¦æ‰±ã†
            sections = [soup.body] if soup.body else [soup]

        # NOTE: éåŒæœŸå‡¦ç†ã§ã¯ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ãŒä¸ŠãŒã‚‰ãšã€ä¸¦åˆ—ãƒ»ä¸¦è¡Œå‡¦ç†ã¯ä¸Šæ‰‹ãå®Ÿè£…ã§ããªã‹ã£ãŸ
        for i, section in enumerate(sections):
            docs.extend(process_html_section((i, section)))

    except Exception as e:
        raise Exception(f"Error processing HTML file: {e}")

    # ãƒšãƒ¼ã‚¸ç•ªå·é †ã«ã‚½ãƒ¼ãƒˆã™ã‚‹
    docs = sorted(docs, key=lambda x: x["metadata"]["page"])

    # ãƒšãƒ¼ã‚¸ç•ªå·ã‚’ãƒ†ã‚­ã‚¹ãƒˆã«åŸ‹ã‚è¾¼ã‚€
    for doc in docs:
        doc["page_content"] += f"<PAGE_NUMBER>{doc['metadata']['page']}</PAGE_NUMBER>"

    return docs


def extract_markdown_text_from_pdf(file: bytes) -> list[dict[str, any]]:
    """PDFãƒ•ã‚¡ã‚¤ãƒ«(.pdf)ã®byteså‹ãƒ‡ãƒ¼ã‚¿ã‹ã‚‰ãƒãƒ¼ã‚¯ãƒ€ã‚¦ãƒ³å½¢å¼ã§ãƒ†ã‚­ã‚¹ãƒˆã‚’æŠ½å‡ºã™ã‚‹

    Args:
        file (bytes): PDFãƒ•ã‚¡ã‚¤ãƒ«(.pdf)ã®byteså‹ãƒ‡ãƒ¼ã‚¿

    Returns:
        list[dict[str, any]]: å„ãƒšãƒ¼ã‚¸ã®ãƒ†ã‚­ã‚¹ãƒˆã¨ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ã‚’å«ã‚€è¾æ›¸ã®ãƒªã‚¹ãƒˆ
    """
    docs = []

    def process_page(i_page):
        i, page = i_page
        page_docs = []
        temp_page_file = None
        try:
            temp_page_file = tempfile.NamedTemporaryFile(delete=False)
            writer = PdfWriter()
            writer.add_page(page)
            # ãƒ¡ãƒ¢ãƒªä¸Šã®ãƒãƒƒãƒ•ã‚¡ã«PDFã‚’æ›¸ãè¾¼ã‚€
            bytes_io = io.BytesIO()
            writer.write(bytes_io)
            # ãƒã‚¤ãƒˆåˆ—ã‚’å–å¾—
            page_bytes = bytes_io.getvalue()
            bytes_io.close()  # ãƒªã‚½ãƒ¼ã‚¹ã‚’è§£æ”¾
            # ãƒã‚¤ãƒˆåˆ—ã‚’ãƒ•ã‚¡ã‚¤ãƒ«ã«æ›¸ãè¾¼ã‚€
            temp_page_file.write(page_bytes)
            temp_page_file.close()  # æ›¸ãè¾¼ã¿çµ‚äº†å¾Œã«é–‰ã˜ã‚‹

            loader = AzureAIDocumentIntelligence().init_loader(
                file_path=temp_page_file.name
            )
            doc = loader.load()
            for d in doc:
                # Document Intelligence ã®ã‚¨ãƒ©ãƒ¼ãƒ¬ã‚¹ãƒãƒ³ã‚¹ã‚’æ¤œå‡º
                if (
                    "ã‚µãƒãƒ¼ãƒˆã•ã‚Œã¦ã„ãªã„ãƒ•ã‚¡ã‚¤ãƒ«å½¢å¼ã§ã™" in d.page_content
                    or d.page_content.strip() == ""
                ):
                    raise Exception(
                        "Document Intelligence returned error or empty content"
                    )

                page_docs.append(
                    {
                        "page_content": d.page_content,
                        "metadata": {"page": i + 1},
                    }
                )

            return page_docs

        except Exception as e:
            print(f"âš ï¸ Document Intelligence failed for page {i}: {e}")
            print("ğŸ”„ Falling back to simple PDF text extraction")
            # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯: ã‚·ãƒ³ãƒ—ãƒ«ãªPDF text extraction
            try:
                text = page.extract_text()
                if text.strip():
                    page_docs.append(
                        {
                            "page_content": f"# Page {i+1}\n\n{text}",
                            "metadata": {"page": i + 1},
                        }
                    )
                else:
                    page_docs.append(
                        {
                            "page_content": f"# Page {i+1}\n\n(No text content found)",
                            "metadata": {"page": i + 1},
                        }
                    )
                return page_docs
            except Exception as fallback_e:
                print(f"âŒ Fallback also failed: {fallback_e}")
                # æœ€å¾Œã®æ‰‹æ®µã¨ã—ã¦ç©ºã®ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã‚’è¿”ã™
                page_docs.append(
                    {
                        "page_content": f"# Page {i+1}\n\n(Text extraction failed)",
                        "metadata": {"page": i + 1},
                    }
                )
                return page_docs

        finally:
            if temp_page_file is not None:
                os.unlink(temp_page_file.name)

    temp_file = tempfile.NamedTemporaryFile(delete=False)
    try:
        temp_file.write(file)
        temp_file.close()  # æ›¸ãè¾¼ã¿çµ‚äº†å¾Œã«é–‰ã˜
        reader = PdfReader(temp_file.name)

        # NOTE: éåŒæœŸå‡¦ç†ã§ã¯ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ãŒä¸ŠãŒã‚‰ãšã€ä¸¦åˆ—ãƒ»ä¸¦è¡Œå‡¦ç†ã¯ä¸Šæ‰‹ãå®Ÿè£…ã§ããªã‹ã£ãŸ
        for i, page in enumerate(reader.pages):
            docs.extend(process_page((i, page)))

    finally:
        os.unlink(temp_file.name)

    # ãƒšãƒ¼ã‚¸ç•ªå·é †ã«ã‚½ãƒ¼ãƒˆã™ã‚‹
    docs = sorted(docs, key=lambda x: x["metadata"]["page"])

    # ãƒšãƒ¼ã‚¸ç•ªå·ã‚’ãƒ†ã‚­ã‚¹ãƒˆã«åŸ‹ã‚è¾¼ã‚€
    for doc in docs:
        doc["page_content"] += (
            "<PAGE_NUMBER>" + str(doc["metadata"]["page"]) + "</PAGE_NUMBER>"
        )

    return docs


def extract_markdown_text_from_image(file: bytes) -> list[dict[str, any]]:
    """ç”»åƒãƒ‡ãƒ¼ã‚¿(.png, .jpg, .jpeg)ã®byteså‹ãƒ‡ãƒ¼ã‚¿ã‹ã‚‰ãƒãƒ¼ã‚¯ãƒ€ã‚¦ãƒ³å½¢å¼ã§ãƒ†ã‚­ã‚¹ãƒˆã‚’æŠ½å‡ºã™ã‚‹

    Args:
        file (bytes): ç”»åƒ(.png, .jpg, .jpeg)ã®bytesãƒ‡ãƒ¼ã‚¿

    Returns:
        list[dict[str, any]]: å„ãƒšãƒ¼ã‚¸ã®ãƒ†ã‚­ã‚¹ãƒˆã¨ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ã‚’å«ã‚€è¾æ›¸ã®ãƒªã‚¹ãƒˆ
    """
    # ç”»åƒãƒ‡ãƒ¼ã‚¿ã‹ã‚‰PDFãƒ‡ãƒ¼ã‚¿ã¸å¤‰æ›
    pdf_bytes = convert_image_to_pdf(file)

    # PDFãƒ‡ãƒ¼ã‚¿ã‹ã‚‰ãƒ†ã‚­ã‚¹ãƒˆæŠ½å‡º
    docs = extract_markdown_text_from_pdf(pdf_bytes)

    return docs


def extract_markdown_text_from_excel(file: bytes):
    """Excelãƒ•ã‚¡ã‚¤ãƒ«(.xlsx, .xls, .xlsm)ã®byteså‹ãƒ‡ãƒ¼ã‚¿ã‹ã‚‰ãƒãƒ¼ã‚¯ãƒ€ã‚¦ãƒ³å½¢å¼ã§ãƒ†ã‚­ã‚¹ãƒˆã‚’æŠ½å‡ºã™ã‚‹

    Args:
        file (bytes): Excelãƒ•ã‚¡ã‚¤ãƒ«(.xlsx, .xls, .xlsm)ã®byteså‹ãƒ‡ãƒ¼ã‚¿

    Returns:
        list[dict[str, any]]: å„ãƒšãƒ¼ã‚¸ã®ãƒ†ã‚­ã‚¹ãƒˆã¨ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ã‚’å«ã‚€è¾æ›¸ã®ãƒªã‚¹ãƒˆ
    """
    # æ—¢å­˜ã®Excelãƒ•ã‚¡ã‚¤ãƒ«ã‚’èª­ã¿è¾¼ã¿
    # BytesIOã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆã«å¤‰æ›ã—ã¦èª­ã¿è¾¼ã‚€
    excel_file = io.BytesIO(file)
    # ãƒ¯ãƒ¼ã‚¯ãƒ–ãƒƒã‚¯èª­ã¿è¾¼ã¿æ™‚ã«ãƒã‚¯ãƒ­ã‚’èª­ã¿è¾¼ã¾ãªã„
    wb = load_workbook(excel_file, keep_vba=False)
    # VBAé–¢é€£ã®å±æ€§ã‚’ã‚¯ãƒªã‚¢
    wb.vba_archive = None
    wb.is_template = False
    wb.vba_modified = False

    # ã‚·ãƒ¼ãƒˆã”ã¨ã®ReadableBufferï¼ˆBytesIOã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆï¼‰ã‚’æ ¼ç´ã™ã‚‹ãƒªã‚¹ãƒˆ
    docs = []

    for i, sheet in enumerate(wb.worksheets):
        print(f"ã‚·ãƒ¼ãƒˆå: {sheet.title}")
        # æ–°ã—ã„ãƒ¯ãƒ¼ã‚¯ãƒ–ãƒƒã‚¯ã‚’ä½œæˆã—ã€ç¾åœ¨ã®ã‚·ãƒ¼ãƒˆã®å†…å®¹ã‚’ã‚³ãƒ”ãƒ¼
        new_wb = Workbook()
        new_ws = new_wb.active
        new_ws.title = sheet.title

        none_count = 0
        max_rows = sheet.max_row
        max_cols = sheet.max_column

        for row_idx in tqdm(range(1, max_rows + 1)):
            none_row = True
            for col_idx in range(1, max_cols + 1):
                cell = sheet.cell(row=row_idx, column=col_idx)
                value = cell.value
                if value is not None:
                    none_row = False
                new_ws.cell(row=row_idx, column=col_idx, value=value)
            if none_row:
                none_count += 1
            else:
                none_count = 0
            # 10è¡Œé€£ç¶šã§ãƒ‡ãƒ¼ã‚¿ãŒç„¡ã‹ã£ãŸã‚‰break
            if none_count > 10:
                break

        # ã‚·ãƒ¼ãƒˆã”ã¨ã«BytesIOã«ä¿å­˜
        sheet_buffer = io.BytesIO()
        new_wb.save(sheet_buffer)
        sheet_buffer.seek(0)

        with tempfile.NamedTemporaryFile(delete=False, suffix=".xlsx") as temp_file:
            temp_file.write(sheet_buffer.getvalue())
            temp_file.flush()
            loader = AzureAIDocumentIntelligence().init_loader(file_path=temp_file.name)
            doc = loader.load()

            docs.append(
                {
                    "page_content": doc[0].page_content,
                    "metadata": {"page": i + 1},
                }
            )

    return docs
