import base64
import json
import os
import re
import traceback
from concurrent.futures import ThreadPoolExecutor, as_completed

from azure.search.documents.indexes.models import *
from langchain.text_splitter import MarkdownHeaderTextSplitter

from src.services.azure_ai_search import AzureAISearch
from src.services.azure_blob_storage import AzureBlobStorage
from src.services.azure_openai import AzureOpenAI
from src.utils.extract_markdown_text_from_file import (
    extract_markdown_text_from_docx,
    extract_markdown_text_from_excel,
    extract_markdown_text_from_html,
    extract_markdown_text_from_image,
    extract_markdown_text_from_pdf,
    extract_markdown_text_from_pptx,
)

japanese_separators = ["\n\n", "  \n", "ã€‚"]
embedding_deploy = os.environ["EMBEDDING_MODEL_NAME"]


# Semantic Chunking
def _semantic_chunk(contents: list):
    """ãƒ†ã‚­ã‚¹ãƒˆã‚’æŒ‡å®šã—ãŸã‚µã‚¤ã‚ºã§åˆ†å‰²ã™ã‚‹"""
    print(f"ğŸ” _semantic_chunk: Processing {len(contents)} content items")

    chunks = []

    content_text = ""
    text_count_array = []
    for i, content in enumerate(contents):
        print(f"ğŸ” Content {i}: {content.keys()}")
        content_text += content["page_content"]
        text_count_array.append(len(content_text))

    print(f"ğŸ” Total content_text length: {len(content_text)}")
    print(f"ğŸ” Content preview: {content_text[:500]}...")

    # Split the document into chunks base on markdown headers.
    headers_to_split_on = [
        ("#", "Header 1"),
        ("##", "Header 2"),
        ("###", "Header 3"),
    ]
    text_splitter = MarkdownHeaderTextSplitter(headers_to_split_on=headers_to_split_on)

    print(f"ğŸ” Using MarkdownHeaderTextSplitter with headers: {headers_to_split_on}")
    chunks = text_splitter.split_text(content_text)
    print(f"ğŸ” MarkdownHeaderTextSplitter result: {len(chunks)} chunks")

    chunks_with_page_number = []
    page_number = 1
    for chunk in chunks:
        keywords = ""
        if "Header 1" in chunk.metadata:
            keywords += chunk.metadata["Header 1"]
        if "Header 2" in chunk.metadata:
            keywords += " " + chunk.metadata["Header 2"]
        if "Header 3" in chunk.metadata:
            keywords += " " + chunk.metadata["Header 3"]

        current_page_number = page_number

        # æ–‡å­—æ•°ãŒå¤šã™ãã‚‹å ´åˆã¯ã€åˆ†å‰²ã™ã‚‹
        if len(chunk.page_content) > 512:
            splitted_chunks = []
            chunk_size = 512
            chunk_overlap = 100
            for i in range(0, len(chunk.page_content), chunk_size):
                start_index = i
                if i > 0:
                    start_index = i - chunk_overlap
                end_index = i + chunk_size
                if end_index > len(chunk.page_content):
                    end_index = len(chunk.page_content)
                splitted_chunks.append(chunk.page_content[start_index:end_index])

            for splitted_chunk in splitted_chunks:
                # ãƒ†ã‚­ã‚¹ãƒˆã«åŸ‹ã‚è¾¼ã¾ã‚ŒãŸãƒšãƒ¼ã‚¸ç•ªå·ã‚’å–å¾—ã™ã‚‹
                matches = re.findall(
                    r"<PAGE_NUMBER>(.*?)</PAGE_NUMBER>", splitted_chunk
                )
                if len(matches) > 0:
                    page_number = int(matches[-1]) + 1
                    splitted_chunk = re.sub(
                        r"<PAGE_NUMBER>(.*?)</PAGE_NUMBER>", "", splitted_chunk
                    )
                # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã®ãƒšãƒ¼ã‚¸ç•ªå·ã‚‚ãƒã‚¤ã‚ºã¨ãªã‚‹ã®ã§å‰Šé™¤ã™ã‚‹
                splitted_chunk = re.sub(
                    r"<!-- PageNumber=(.*?) -->", "", splitted_chunk
                )

                chunks_with_page_number.append(
                    {
                        "content": splitted_chunk,
                        "page_number": current_page_number,
                        "keywords": keywords,
                    }
                )
        else:
            # ãƒ†ã‚­ã‚¹ãƒˆã«åŸ‹ã‚è¾¼ã¾ã‚ŒãŸãƒšãƒ¼ã‚¸ç•ªå·ã‚’å–å¾—ã™ã‚‹
            matches = re.findall(
                r"<PAGE_NUMBER>(.*?)</PAGE_NUMBER>", chunk.page_content
            )
            if len(matches) > 0:
                page_number = int(matches[-1]) + 1
                chunk.page_content = re.sub(
                    r"<PAGE_NUMBER>(.*?)</PAGE_NUMBER>", "", chunk.page_content
                )
            # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã®ãƒšãƒ¼ã‚¸ç•ªå·ã‚‚ãƒã‚¤ã‚ºã¨ãªã‚‹ã®ã§å‰Šé™¤ã™ã‚‹
            chunk.page_content = re.sub(
                r"<!-- PageNumber=(.*?) -->", "", chunk.page_content
            )

            chunks_with_page_number.append(
                {
                    "content": chunk.page_content,
                    "page_number": current_page_number,
                    "keywords": keywords,
                }
            )

    print(f"ğŸ” Final result: {len(chunks_with_page_number)} chunks with page numbers")
    if chunks_with_page_number:
        print(f"ğŸ” Sample chunk: {chunks_with_page_number[0]}")

    return chunks_with_page_number


# Semantic Chunking From Excel
def _semantic_chunk_from_excel(contents: list):
    """ãƒ†ã‚­ã‚¹ãƒˆã‚’æŒ‡å®šã—ãŸã‚µã‚¤ã‚ºã§åˆ†å‰²ã™ã‚‹"""
    # Split the document into chunks base on markdown headers.
    headers_to_split_on = [
        ("#", "Header 1"),
        ("##", "Header 2"),
        ("###", "Header 3"),
    ]
    text_splitter = MarkdownHeaderTextSplitter(headers_to_split_on=headers_to_split_on)

    chunks_with_page_number = []

    for content in contents:
        chunks = []
        chunks = text_splitter.split_text(content["page_content"])

        for chunk in chunks:
            keywords = ""
            if "Header 1" in chunk.metadata:
                keywords += chunk.metadata["Header 1"]
            if "Header 2" in chunk.metadata:
                keywords += " " + chunk.metadata["Header 2"]
            if "Header 3" in chunk.metadata:
                keywords += " " + chunk.metadata["Header 3"]

        chunks_with_page_number.append(
            {
                "content": chunk.page_content,
                "page_number": content["metadata"]["page"],
                "keywords": keywords,
            }
        )

    return chunks_with_page_number


# ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹å‡¦ç†ã‚’ãƒãƒƒãƒåŒ–
def _index_docs_to_azure_ai_search(
    chunks,
    source_file_name: str,
    index_type: str,
    actual_blob_name: str = None,
    batch_size=10,
):
    """ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã‚’Azure AI Searchã«ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã—ã€ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã«ã‚‚è¨˜éŒ²ã™ã‚‹"""
    index_name = AzureAISearch().get_index_name(index_type)
    search_client = AzureAISearch().init_search_client(index_name)
    open_ai_client = AzureOpenAI().init_client()

    def _embed_and_prepare_document(open_ai_client, chunk, source_file_name, i):
        try:
            response = open_ai_client.embeddings.create(
                input=chunk["content"], model=embedding_deploy
            )
            document = {
                "id": _encode_data(source_file_name + "_" + str(i)),
                "keywords": chunk["keywords"],
                "content": chunk["content"],
                "contentVector": response.data[0].embedding,
                "pageNumber": chunk["page_number"],
                "sourceFileName": source_file_name,
                "blobUrl": AzureBlobStorage().get_blob_url(source_file_name),
            }
            print(document)
            return document
        except Exception as e:
            raise Exception(f"Error embedding chunk {i}: {e}")

    batch = []
    with ThreadPoolExecutor(max_workers=5) as executor:
        futures = []
        for i, chunk in enumerate(chunks):
            futures.append(
                executor.submit(
                    _embed_and_prepare_document,
                    open_ai_client,
                    chunk,
                    source_file_name,
                    i,
                )
            )
            if len(futures) >= batch_size:
                for future in as_completed(futures):
                    document = future.result()
                    batch.append(document)
                    del future
                search_client.upload_documents(documents=batch)
                batch.clear()
                futures.clear()
        # æ®‹ã‚Šã®ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã‚’ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰
        if futures:
            for future in as_completed(futures):
                document = future.result()
                batch.append(document)
                del future
            search_client.upload_documents(documents=batch)
            batch.clear()

    # ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹å‡¦ç†ãŒå®Œäº†ã—ãŸã‚‰ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã«è¨˜éŒ²
    # actual_blob_nameã«ã¯å®Ÿéš›ã«Blob Storageã«ä¿å­˜ã•ã‚ŒãŸãƒ•ã‚¡ã‚¤ãƒ«åï¼ˆã‚¿ã‚¤ãƒ ã‚¹ã‚¿ãƒ³ãƒ—ä»˜ãï¼‰ãŒå«ã¾ã‚Œã‚‹
    blob_name_to_save = actual_blob_name if actual_blob_name else source_file_name
    _save_indexed_file_to_database(blob_name_to_save, index_name, index_type)


def _save_indexed_file_to_database(
    original_blob_name: str, indexed_blob_name: str, index_type: str
):
    """ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹æ¸ˆã¿ãƒ•ã‚¡ã‚¤ãƒ«æƒ…å ±ã‚’ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã«ä¿å­˜"""
    try:
        import os

        from src.schemas.index_schema import IndexedFileTable
        from src.services.db import get_session

        # ãƒ•ã‚¡ã‚¤ãƒ«æ‹¡å¼µå­ã‹ã‚‰ file_type ã‚’åˆ¤å®š
        file_extension = os.path.splitext(original_blob_name)[1].lower()
        file_type_map = {
            ".pdf": "PDF",
            ".docx": "DOCX",
            ".doc": "DOC",
            ".pptx": "PPTX",
            ".xlsx": "EXCEL",
            ".xls": "EXCEL",
            ".xlsm": "EXCEL",
            ".html": "HTML",
            ".htm": "HTML",
            ".png": "IMAGE",
            ".jpg": "IMAGE",
            ".jpeg": "IMAGE",
        }
        file_type = file_type_map.get(file_extension, "UNKNOWN")

        with get_session() as session:
            # æ—¢å­˜ãƒ¬ã‚³ãƒ¼ãƒ‰ã‚’ãƒã‚§ãƒƒã‚¯
            existing = (
                session.query(IndexedFileTable)
                .filter(IndexedFileTable.original_blob_name == original_blob_name)
                .first()
            )

            if existing:
                # æ—¢å­˜ãƒ¬ã‚³ãƒ¼ãƒ‰ã‚’æ›´æ–°
                existing.indexed_blob_name = indexed_blob_name
                existing.index_type = index_type
                existing.file_type = file_type
                print(f"âœ… Updated indexed file record: {original_blob_name}")
            else:
                # æ–°è¦ãƒ¬ã‚³ãƒ¼ãƒ‰ã‚’ä½œæˆ
                indexed_file = IndexedFileTable(
                    original_blob_name=original_blob_name,
                    indexed_blob_name=indexed_blob_name,
                    file_type=file_type,
                    index_type=index_type,
                )
                session.add(indexed_file)
                print(f"âœ… Created indexed file record: {original_blob_name}")

    except Exception as e:
        print(f"âŒ Error saving indexed file to database: {str(e)}")
        # ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¦ã‚‚ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹å‡¦ç†ã¯æˆåŠŸã¨ã™ã‚‹


def _encode_data(data: str) -> str:
    return base64.urlsafe_b64encode(data.encode("utf-8")).decode("utf-8")


def _decode_data(encoded_data: str) -> str:
    return base64.urlsafe_b64decode(encoded_data).decode("utf-8")


async def index_pdf_docs(fileBytes: bytes, fileName: str, index_type: str):
    """PDFï¼ˆ.pdfï¼‰ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã‚’ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã™ã‚‹"""
    try:
        print(f"ğŸ”„ Starting PDF indexing for: {fileName}")

        # ã¾ãšBlobã«ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰
        blob_storage = AzureBlobStorage()
        upload_result = await blob_storage.upload_document(
            fileBytes, fileName, "application/pdf"
        )
        print(f"âœ… Blob uploaded: {upload_result}")

        # PDFã‹ã‚‰ãƒ†ã‚­ã‚¹ãƒˆæŠ½å‡º
        print("ğŸ”„ Extracting text from PDF...")
        content = extract_markdown_text_from_pdf(fileBytes)
        print(f"ğŸ“„ Extracted {len(content)} pages of content")
        if content:
            print(f"ğŸ“ Sample content: {content[0]['page_content'][:200]}...")

        # ãƒãƒ£ãƒ³ã‚¯ç”Ÿæˆ
        print("ğŸ”„ Creating semantic chunks...")
        chunks = _semantic_chunk(content)
        print(f"ğŸ“¦ Generated {len(chunks)} chunks")

        # AI Searchã«ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹
        if chunks:
            print("ğŸ”„ Indexing to AI Search...")
            _index_docs_to_azure_ai_search(
                chunks, fileName, index_type, upload_result["blob_name"]
            )
            print("âœ… AI Search indexing completed")
        else:
            print("âš ï¸ No chunks generated, skipping AI Search indexing")

        return {
            "status": "success",
            "message": "PDFãƒ•ã‚¡ã‚¤ãƒ«ã®ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹åŒ–ãŒå®Œäº†ã—ã¾ã—ãŸ",
            "processed_chunks": len(chunks),
            "filename": fileName,
            "index_type": index_type,
            "blob_uploaded": True,
            "content_pages": len(content) if content else 0,
        }
    except Exception as e:
        print("âŒ index_pdf_docs error:")
        print(traceback.format_exc())
        return {
            "status": "error",
            "message": f"PDFã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹åŒ–ä¸­ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {str(e)}",
            "filename": fileName,
        }


async def index_docx_docs(fileBytes: bytes, fileName: str, index_type: str):
    """Wordï¼ˆ.docxï¼‰ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã‚’ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã™ã‚‹"""
    try:
        print(f"ğŸ”„ Starting DOCX indexing for: {fileName}")

        # ã¾ãšBlobã«ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰
        blob_storage = AzureBlobStorage()

        upload_result = await blob_storage.upload_document(
            fileBytes,
            fileName,
            "application/vnd.openxmlformats-officedocument.wordprocessingml.document",
        )
        print(f"âœ… Blob uploaded: {upload_result}")

        # Wordã‹ã‚‰ãƒ†ã‚­ã‚¹ãƒˆæŠ½å‡º
        print("ğŸ”„ Extracting text from DOCX...")
        content = extract_markdown_text_from_docx(fileBytes)
        print("ğŸ“„ Extracted content from DOCX")

        # ãƒãƒ£ãƒ³ã‚¯ç”Ÿæˆ
        print("ğŸ”„ Creating semantic chunks...")
        chunks = _semantic_chunk(content)
        print(f"ğŸ“¦ Generated {len(chunks)} chunks")

        # AI Searchã«ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹
        if chunks:
            print("ğŸ”„ Indexing to AI Search...")
            _index_docs_to_azure_ai_search(
                chunks, fileName, index_type, upload_result["blob_name"]
            )
            print("âœ… AI Search indexing completed")
        else:
            print("âš ï¸ No chunks generated, skipping AI Search indexing")

        return {
            "status": "success",
            "message": "Wordãƒ•ã‚¡ã‚¤ãƒ«ã®ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹åŒ–ãŒå®Œäº†ã—ã¾ã—ãŸ",
            "processed_chunks": len(chunks),
            "filename": fileName,
            "index_type": index_type,
            "blob_uploaded": True,
        }
    except Exception as e:
        print("âŒ index_docx_docs error:")
        import traceback

        print(traceback.format_exc())
        return {
            "status": "error",
            "message": f"Wordã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹åŒ–ä¸­ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {str(e)}",
            "filename": fileName,
        }


async def index_pptx_docs(fileBytes: bytes, fileName: str, index_type: str):
    """Power Pointï¼ˆ.pptxï¼‰ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã‚’ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã™ã‚‹"""
    try:
        print(f"ğŸ”„ Starting PPTX indexing for: {fileName}")

        # ã¾ãšBlobã«ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰
        blob_storage = AzureBlobStorage()

        upload_result = await blob_storage.upload_document(
            fileBytes,
            fileName,
            "application/vnd.openxmlformats-officedocument.presentationml.presentation",
        )
        print(f"âœ… Blob uploaded: {upload_result}")

        # PowerPointã‹ã‚‰ãƒ†ã‚­ã‚¹ãƒˆæŠ½å‡º
        print("ğŸ”„ Extracting text from PPTX...")
        content = extract_markdown_text_from_pptx(fileBytes)
        print("ğŸ“„ Extracted content from PPTX")

        # ãƒãƒ£ãƒ³ã‚¯ç”Ÿæˆ
        print("ğŸ”„ Creating semantic chunks...")
        chunks = _semantic_chunk(content)
        print(f"ğŸ“¦ Generated {len(chunks)} chunks")

        # AI Searchã«ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹
        if chunks:
            print("ğŸ”„ Indexing to AI Search...")
            _index_docs_to_azure_ai_search(
                chunks, fileName, index_type, upload_result["blob_name"]
            )
            print("âœ… AI Search indexing completed")
        else:
            print("âš ï¸ No chunks generated, skipping AI Search indexing")

        return {
            "status": "success",
            "message": "PowerPointãƒ•ã‚¡ã‚¤ãƒ«ã®ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹åŒ–ãŒå®Œäº†ã—ã¾ã—ãŸ",
            "processed_chunks": len(chunks),
            "filename": fileName,
            "index_type": index_type,
            "blob_uploaded": True,
        }
    except Exception as e:
        print("âŒ index_pptx_docs error:")
        import traceback

        print(traceback.format_exc())
        return {
            "status": "error",
            "message": f"PowerPointã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹åŒ–ä¸­ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {str(e)}",
            "filename": fileName,
        }


async def index_html_docs(fileBytes: bytes, fileName: str, index_type: str):
    """HTMLï¼ˆ.htmlï¼‰ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã‚’ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã™ã‚‹"""
    try:
        print(f"ğŸ”„ Starting HTML indexing for: {fileName}")

        # ã¾ãšBlobã«ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰
        blob_storage = AzureBlobStorage()

        upload_result = await blob_storage.upload_document(
            fileBytes, fileName, "text/html"
        )
        print(f"âœ… Blob uploaded: {upload_result}")

        # HTMLã‹ã‚‰ãƒ†ã‚­ã‚¹ãƒˆæŠ½å‡º
        print("ğŸ”„ Extracting text from HTML...")
        content = extract_markdown_text_from_html(fileBytes)
        print("ğŸ“„ Extracted content from HTML")

        # ãƒãƒ£ãƒ³ã‚¯ç”Ÿæˆ
        print("ğŸ”„ Creating semantic chunks...")
        chunks = _semantic_chunk(content)
        print(f"ğŸ“¦ Generated {len(chunks)} chunks")

        # AI Searchã«ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹
        if chunks:
            print("ğŸ”„ Indexing to AI Search...")
            _index_docs_to_azure_ai_search(
                chunks, fileName, index_type, upload_result["blob_name"]
            )
            print("âœ… AI Search indexing completed")
        else:
            print("âš ï¸ No chunks generated, skipping AI Search indexing")

        return {
            "status": "success",
            "message": "HTMLãƒ•ã‚¡ã‚¤ãƒ«ã®ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹åŒ–ãŒå®Œäº†ã—ã¾ã—ãŸ",
            "processed_chunks": len(chunks),
            "filename": fileName,
            "index_type": index_type,
            "blob_uploaded": True,
        }
    except Exception as e:
        print("âŒ index_html_docs error:")
        import traceback

        print(traceback.format_exc())
        return {
            "status": "error",
            "message": f"HTMLã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹åŒ–ä¸­ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {str(e)}",
            "filename": fileName,
        }


async def index_image_docs(fileBytes: bytes, fileName: str, index_type: str):
    """ç”»åƒï¼ˆ.png, .jpg, .jpegï¼‰ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã‚’ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã™ã‚‹"""
    try:
        print(f"ğŸ”„ Starting image indexing for: {fileName}")

        # ã¾ãšBlobã«ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰
        blob_storage = AzureBlobStorage()

        file_extension = os.path.splitext(fileName)[1].lower()
        content_type = "image/png" if file_extension == ".png" else "image/jpeg"

        upload_result = await blob_storage.upload_document(
            fileBytes, fileName, content_type
        )
        print(f"âœ… Blob uploaded: {upload_result}")

        # ç”»åƒã‹ã‚‰ãƒ†ã‚­ã‚¹ãƒˆæŠ½å‡º
        print("ğŸ”„ Extracting text from image...")
        content = extract_markdown_text_from_image(fileBytes)
        print("ğŸ“„ Extracted content from image")

        # ãƒãƒ£ãƒ³ã‚¯ç”Ÿæˆ
        print("ğŸ”„ Creating semantic chunks...")
        chunks = _semantic_chunk(content)
        print(f"ğŸ“¦ Generated {len(chunks)} chunks")

        # AI Searchã«ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹
        if chunks:
            print("ğŸ”„ Indexing to AI Search...")
            _index_docs_to_azure_ai_search(
                chunks, fileName, index_type, upload_result["blob_name"]
            )
            print("âœ… AI Search indexing completed")
        else:
            print("âš ï¸ No chunks generated, skipping AI Search indexing")

        return {
            "status": "success",
            "message": "ç”»åƒãƒ•ã‚¡ã‚¤ãƒ«ã®ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹åŒ–ãŒå®Œäº†ã—ã¾ã—ãŸ",
            "processed_chunks": len(chunks),
            "filename": fileName,
            "index_type": index_type,
            "blob_uploaded": True,
        }
    except Exception as e:
        print("âŒ index_image_docs error:")
        import traceback

        print(traceback.format_exc())
        return {
            "status": "error",
            "message": f"ç”»åƒã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹åŒ–ä¸­ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {str(e)}",
            "filename": fileName,
        }


async def index_excel_docs(fileBytes: bytes, fileName: str, index_type: str):
    """Excelï¼ˆ.xlsx, .xls, .xlsmï¼‰ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã‚’ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã™ã‚‹"""
    try:
        print(f"ğŸ”„ Starting Excel indexing for: {fileName}")

        # ã¾ãšBlobã«ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰
        blob_storage = AzureBlobStorage()

        upload_result = await blob_storage.upload_document(
            fileBytes,
            fileName,
            "application/vnd.openxmlformats-officedocument.spreadsheetml.sheet",
        )
        print(f"âœ… Blob uploaded: {upload_result}")

        # Excelã‹ã‚‰ãƒ†ã‚­ã‚¹ãƒˆæŠ½å‡º
        print("ğŸ”„ Extracting text from Excel...")
        content = extract_markdown_text_from_excel(fileBytes)
        print("ğŸ“„ Extracted content from Excel")

        # ãƒãƒ£ãƒ³ã‚¯ç”Ÿæˆ
        print("ğŸ”„ Creating semantic chunks...")
        chunks = _semantic_chunk(content)
        print(f"ğŸ“¦ Generated {len(chunks)} chunks")

        # AI Searchã«ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹
        if chunks:
            print("ğŸ”„ Indexing to AI Search...")
            _index_docs_to_azure_ai_search(
                chunks, fileName, index_type, upload_result["blob_name"]
            )
            print("âœ… AI Search indexing completed")
        else:
            print("âš ï¸ No chunks generated, skipping AI Search indexing")

        return {
            "status": "success",
            "message": "Excelãƒ•ã‚¡ã‚¤ãƒ«ã®ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹åŒ–ãŒå®Œäº†ã—ã¾ã—ãŸ",
            "processed_chunks": len(chunks),
            "filename": fileName,
            "index_type": index_type,
            "blob_uploaded": True,
        }
    except Exception as e:
        print("âŒ index_excel_docs error:")
        import traceback

        print(traceback.format_exc())
        return {
            "status": "error",
            "message": f"Excelã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹åŒ–ä¸­ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {str(e)}",
            "filename": fileName,
        }


def index_videostep_docs(fileBytes: bytes, fileName: str, index_type: str):
    """å‹•ç”»ãƒãƒ‹ãƒ¥ã‚¢ãƒ«ãƒ‡ãƒ¼ã‚¿ï¼ˆ.jsonï¼‰ã‚’ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã™ã‚‹ã€‚1ãƒ•ã‚¡ã‚¤ãƒ«ã™ã¹ã¦ã‚’txtã¨ã—ã¦æ‰±ã†"""
    print(1)
    try:
        json_content = json.loads(fileBytes.decode("utf-8"))
    except Exception:
        print("json.load error:")
        print(traceback.format_exc())
        raise Exception("json.load error:")
    print(2)

    try:
        # JSONãƒ‡ãƒ¼ã‚¿ã‚’_semantic_chunkãŒæœŸå¾…ã™ã‚‹å½¢å¼ã«å¤‰æ›
        # JSONãƒ‡ãƒ¼ã‚¿ã‚’æ–‡å­—åˆ—ã«å¤‰æ›ã—ã€ãã‚Œã‚’page_contentã¨ã—ã¦æŒã¤è¾æ›¸ã‚’ä½œæˆ
        content = [{"page_content": json.dumps(json_content, ensure_ascii=False)}]
        chunks = _semantic_chunk(content)
    except Exception:
        print("_semantic_chunk error:")
        print(traceback.format_exc())
        raise Exception("_semantic_chunk error:")

    print(chunks)
    print(fileName)
    print(index_type)
    _index_docs_to_azure_ai_search(chunks, fileName, index_type)
